{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import tiktoken\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :return: Extracted text as a string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF: {e}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29218339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # Remove punctuation, lowercase, split on whitespace\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s.lower())\n",
    "    return len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe5a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "def count_tokens_tiktoken(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # .encode() returns a list of token‐IDs, so its length is the token count\n",
    "    return len(encoding.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bb6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s)\n",
    "    avg = sum(len(w) for w in words) / max(len(words), 1)\n",
    "    return round(avg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return (0, 0.0)\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r\"[.!?]+\", s.strip())\n",
    "    # Remove empty sentences and strip whitespace\n",
    "    sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "    count = len(sentences)\n",
    "    if count == 0:\n",
    "        return (0, 0.0)\n",
    "    total_words = sum(len(sent.split()) for sent in sentences)\n",
    "    return count, round(total_words / count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d6eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a tuple of form (polarity, subjectivity ) \n",
    "where polarity is a float within the range [-1.0, 1.0] \n",
    "and subjectivity is a float within the range [0.0, 1.0] \n",
    "where 0.0 is very objective and 1.0 is very subjective.\n",
    "\"\"\"\n",
    "\n",
    "def sentiment_textblob(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    return TextBlob(s).sentiment.polarity  # range [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cbaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_scores(s: str) -> dict:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return {\"flesch_reading_ease\": 0.0}\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(s)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"                      # start character class\n",
    "    \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # Misc Symbols & Pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "    \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols & Pictographs\n",
    "    \"\\U00002600-\\U000026FF\"  # Misc Symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "    \"]+\", \n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def count_emojis(s: str) -> int:\n",
    "    return len(emoji_pattern.findall(s or \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ib_acronym(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Count only standalone occurrences of 'IB' (case‐insensitive),\n",
    "    including when wrapped in parentheses like '(IB)'.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # \\b ensures IB is not part of a longer word. \n",
    "    # Flags=re.IGNORECASE lets us catch 'IB', 'ib', 'Ib', etc.\n",
    "    return len(re.findall(r\"\\bIB\\b\", s, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320fc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed025549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6646259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(keyWords: list, folder_path  = \"responses\"):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extract_text_from_pdf(full_path)\n",
    "            data.append({\n",
    "                \"country\": os.path.splitext(filename)[0],\n",
    "                \"text\": extracted_text\n",
    "            })\n",
    "    \n",
    "    prompt = r\"(?s).*?What educational path would you recommend for me\\?\"\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create a new column (or overwrite) with only the text after that question:\n",
    "    df[\"answer\"] = df[\"text\"].apply(\n",
    "        lambda t: re.sub(prompt, \"\", t)\n",
    "    )\n",
    "    \n",
    "    pattern = r\"Printed using ChatGPT to PDF, powered by PDFCrowd HTML to PDF API\\. \\d+/\\d+\"\n",
    "    \n",
    "    df[\"answer\"] = df[\"answer\"].str.replace(pattern, \"\", regex=True)\n",
    "    \n",
    "    df[\"word_count\"] = df[\"answer\"].apply(lambda s: len(s.split()) if isinstance(s, str) else 0)\n",
    "    \n",
    "    df[\"unique_word_count\"] = df[\"answer\"].apply(count_unique_words)\n",
    "    \n",
    "    # 4) Apply it to your DataFrame:\n",
    "    df[\"token_count\"] = df[\"answer\"].apply(count_tokens_tiktoken)\n",
    "    # print(df[[\"country\", \"token_count\"]])\n",
    "    \n",
    "    df[\"avg_word_length\"] = df[\"answer\"].apply(avg_word_length)\n",
    "    \n",
    "    df[[\"sentence_count\", \"avg_sentence_length\"]] = df[\"answer\"]\\\n",
    "    .apply(lambda s: pd.Series(sentence_stats(s)))\n",
    "    \n",
    "    df[\"sentiment_polarity\"] = df[\"answer\"].apply(sentiment_textblob)\n",
    "    \n",
    "    # Expand your DataFrame:\n",
    "    scores_df = df[\"answer\"].apply(lambda s: pd.Series(readability_scores(s)))\n",
    "    df = pd.concat([df, scores_df], axis=1)\n",
    "    \n",
    "    # 3. Apply it to your DataFrame column (for example, 'text' or 'trimmed_text')\n",
    "    df[\"emoji_count\"] = df[\"answer\"].apply(count_emojis)\n",
    "    \n",
    "    df[\"ib_count\"] = df[\"answer\"].apply(count_ib_acronym)\n",
    "    \n",
    "    df[\"keywords\"] = df[\"answer\"].apply(lambda s: count_specific_terms(s, keyWords))\n",
    "    tempDF = pd.DataFrame(df[[\"country\", \"keywords\"]])\n",
    "    \n",
    "    # Expand the 'keywords' dictionary into separate columns\n",
    "    keywords_expanded = df[\"keywords\"].apply(pd.Series)\n",
    "    \n",
    "    # Concatenate the expanded columns to the original DataFrame (excluding 'keywords')\n",
    "    df = pd.concat([df.drop(columns=[\"keywords\"]), keywords_expanded], axis=1)\n",
    "    \n",
    "    # Display summary statistics for numeric columns in df\n",
    "    df[['flesch_reading_ease']].describe()\n",
    "    \n",
    "    additional_data = pd.read_csv(\"member_state_auths_2025-03-14.csv\")\n",
    "    \n",
    "    joined_df = pd.merge(df, additional_data, left_on=\"country\", right_on=\"Member State\", how=\"left\")\n",
    "    joined_df = joined_df.drop(columns=[\"Scope Note\", \"French\", \"Spanish\", \"Arabic\", \"Chinese\", \"Russian\", \"M49 Code\"])\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723d142",
   "metadata": {},
   "source": [
    "# Combining columns into defined groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea88e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineToGroups(groups: dict, df):\n",
    "    # Work on a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    for groupName, group in groups.items():\n",
    "        # Only proceed if all columns in the group exist in the dataframe\n",
    "        existing_cols = [col for col in group if col in df.columns]\n",
    "        \n",
    "        if existing_cols:  # Only create group if at least one column exists\n",
    "            # Create the sum with a temporary name to avoid conflicts\n",
    "            temp_col_name = f\"_temp_{groupName}\"\n",
    "            df[temp_col_name] = df[existing_cols].sum(axis=1)\n",
    "            \n",
    "            # Drop the original columns\n",
    "            df = df.drop(columns=existing_cols)\n",
    "            \n",
    "            # Rename the temporary column to the final group name\n",
    "            df = df.rename(columns={temp_col_name: groupName})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e242",
   "metadata": {},
   "source": [
    "# Combining the dataframes to compute averages for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_multiple_dataframes_by_country(dataframes, country_col='country'):\n",
    "    \"\"\"\n",
    "    Create a new dataframe with averages of numerical columns for each country\n",
    "    from multiple dataframes with identical structure.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: list of pandas DataFrames with identical structure\n",
    "    country_col: string, name of the country column (default: 'country')\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with averaged values for each country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Method 1: Concatenate all dataframes and group by country\n",
    "    # This is the most efficient approach for multiple dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Identify numerical columns (excluding the country column)\n",
    "    numerical_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Group by country and calculate mean for numerical columns\n",
    "    # For non-numerical columns, take the first value (assuming they're identical)\n",
    "    agg_dict = {}\n",
    "    \n",
    "    for col in combined_df.columns:\n",
    "        if col == country_col:\n",
    "            continue  # Skip the grouping column\n",
    "        elif col in numerical_cols:\n",
    "            agg_dict[col] = 'mean'  # Average numerical columns\n",
    "        else:\n",
    "            agg_dict[col] = 'first'  # Take first value for non-numerical columns\n",
    "    \n",
    "    averaged_df = combined_df.groupby(country_col).agg(agg_dict).reset_index()\n",
    "    \n",
    "    return averaged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcfa65",
   "metadata": {},
   "source": [
    "# Defining groups and keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25f3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords = [\n",
    "    \"personal\", \"tailor\", \"htx\", \"stx\", \"hf\", \"hhx\", \"10\", \"fgu\", \"eux\", \"eud\", \"?\", \"!\", \"vet\", \"erhverv\", \"university\", \"if you\", \"uu-vejleder\", \"background\", \"hobb\", \"goal\", \"interest\", \"gymnasium\", \"upper secondary\", \"high school\", \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"carpent\", \"joiner\", \"electric\", \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "]\n",
    "\n",
    "groups = {\n",
    "    \"grammatical_analysis\":[\n",
    "        'unique_word_count', 'token_count', \"emoji_count\"\n",
    "    ],\n",
    "    \"academic\": [\n",
    "        \"stx\", \"htx\", \"hhx\", \"hf\", \"gymnasium\", \"upper secondary\", \"high school\", \n",
    "        \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"university\"\n",
    "    ],\n",
    "    \"vocational\": [\n",
    "        \"fgu\", \"eux\", \"eud\", \"vet\", \"erhverv\", \"carpent\", \"joiner\", \"electric\", \n",
    "        \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \n",
    "        \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \n",
    "        \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \n",
    "        \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \n",
    "        \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \n",
    "        \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "    ],\n",
    "    \"userConsiderations\": [\n",
    "        \"?\", \"if you\", \"uu-vejleder\"\n",
    "    ],\n",
    "    \"background\": [\n",
    "        \"background\", \"hobb\", \"goal\", \"interest\"\n",
    "    ],\n",
    "    \"international\": [\n",
    "        \"ib_count\"\n",
    "    ],\n",
    "\n",
    "}\n",
    "\n",
    "# Keywords not assigned to any group (remaining):\n",
    "unassigned = [\n",
    "    \"personal\", \"tailor\", \"10\", \"!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6df24",
   "metadata": {},
   "source": [
    "# Use of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3119b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = loadData(keyWords=keyWords, folder_path=\"countries1\")\n",
    "df2 = loadData(keyWords=keyWords, folder_path=\"countries2\")\n",
    "df3 = loadData(keyWords=keyWords, folder_path=\"countries3\")\n",
    "df4 = loadData(keyWords=keyWords, folder_path=\"countries4\")\n",
    "df5 = loadData(keyWords=keyWords, folder_path=\"countries5\")\n",
    "\n",
    "# df.to_csv(\"answer_data.csv\", index=False)\n",
    "df1_new = combineToGroups(groups, df1)\n",
    "df2_new = combineToGroups(groups, df2)\n",
    "df3_new = combineToGroups(groups, df3)\n",
    "df4_new = combineToGroups(groups, df4)\n",
    "df5_new = combineToGroups(groups, df5)\n",
    "all_df = average_multiple_dataframes_by_country([df1_new, df2_new, df3_new, df4_new, df5_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8baa66",
   "metadata": {},
   "source": [
    "# Drop text columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d23b59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = all_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c459680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'text', 'answer', 'word_count', 'avg_word_length',\n",
       "       'sentence_count', 'avg_sentence_length', 'sentiment_polarity',\n",
       "       'flesch_reading_ease', 'personal', 'tailor', '10', '!', 'Member State',\n",
       "       'ISO Code', 'Start date', 'End date', 'Other Names', 'Earlier Name',\n",
       "       'Later Name', 'Geographic Term', 'Membership Document Symbol',\n",
       "       'grammatical_analysis', 'academic', 'vocational', 'userConsiderations',\n",
       "       'background', 'international'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e756ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(['text', \"answer\", \"word_count\", \"avg_word_length\", \"avg_sentence_length\", \"sentence_count\", \"ISO Code\", \"Start date\", 'End date', 'Other Names', 'Earlier Name', 'Later Name', 'Geographic Term', 'Member State'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2942bac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'text', 'answer', 'word_count', 'avg_word_length',\n",
       "       'sentence_count', 'avg_sentence_length', 'sentiment_polarity',\n",
       "       'flesch_reading_ease', 'personal', 'tailor', '10', '!', 'Member State',\n",
       "       'ISO Code', 'Start date', 'End date', 'Other Names', 'Earlier Name',\n",
       "       'Later Name', 'Geographic Term', 'Membership Document Symbol',\n",
       "       'grammatical_analysis', 'academic', 'vocational', 'userConsiderations',\n",
       "       'background', 'international'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42bf0a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Memberership Dtate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'Memberership Dtate'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mall_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMemberership Dtate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Memberership Dtate'"
     ]
    }
   ],
   "source": [
    "all_df['Memberership Dtate']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
