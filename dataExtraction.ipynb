{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8745ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import os\n",
    "import tiktoken\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ebeb12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :return: Extracted text as a string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF: {e}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29218339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # Remove punctuation, lowercase, split on whitespace\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s.lower())\n",
    "    return len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8fe5a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "def count_tokens_tiktoken(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # .encode() returns a list of token‐IDs, so its length is the token count\n",
    "    return len(encoding.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25bb6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s)\n",
    "    avg = sum(len(w) for w in words) / max(len(words), 1)\n",
    "    return round(avg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4b1a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return (0, 0.0)\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r\"[.!?]+\", s.strip())\n",
    "    # Remove empty sentences and strip whitespace\n",
    "    sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "    count = len(sentences)\n",
    "    if count == 0:\n",
    "        return (0, 0.0)\n",
    "    total_words = sum(len(sent.split()) for sent in sentences)\n",
    "    return count, round(total_words / count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d6d6eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a tuple of form (polarity, subjectivity ) \n",
    "where polarity is a float within the range [-1.0, 1.0] \n",
    "and subjectivity is a float within the range [0.0, 1.0] \n",
    "where 0.0 is very objective and 1.0 is very subjective.\n",
    "\"\"\"\n",
    "\n",
    "def sentiment_textblob(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    return TextBlob(s).sentiment.polarity  # range [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "60cbaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_scores(s: str) -> dict:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return {\"flesch_reading_ease\": 0.0}\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(s)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "18b5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"                      # start character class\n",
    "    \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # Misc Symbols & Pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "    \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols & Pictographs\n",
    "    \"\\U00002600-\\U000026FF\"  # Misc Symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "    \"]+\", \n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def count_emojis(s: str) -> int:\n",
    "    return len(emoji_pattern.findall(s or \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "292cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ib_acronym(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Count only standalone occurrences of 'IB' (case‐insensitive),\n",
    "    including when wrapped in parentheses like '(IB)'.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # \\b ensures IB is not part of a longer word. \n",
    "    # Flags=re.IGNORECASE lets us catch 'IB', 'ib', 'Ib', etc.\n",
    "    return len(re.findall(r\"\\bIB\\b\", s, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "320fc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ed025549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b6646259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(keyWords: list, folder_path  = \"responses\"):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extract_text_from_pdf(full_path)\n",
    "            data.append({\n",
    "                \"country\": os.path.splitext(filename)[0],\n",
    "                \"text\": extracted_text\n",
    "            })\n",
    "    \n",
    "    prompt = r\"(?s).*?What educational path would you recommend for me\\?\"\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create a new column (or overwrite) with only the text after that question:\n",
    "    df[\"answer\"] = df[\"text\"].apply(\n",
    "        lambda t: re.sub(prompt, \"\", t)\n",
    "    )\n",
    "    \n",
    "    pattern = r\"Printed using ChatGPT to PDF, powered by PDFCrowd HTML to PDF API\\. \\d+/\\d+\"\n",
    "    \n",
    "    df[\"answer\"] = df[\"answer\"].str.replace(pattern, \"\", regex=True)\n",
    "    \n",
    "    df[\"word_count\"] = df[\"answer\"].apply(lambda s: len(s.split()) if isinstance(s, str) else 0)\n",
    "    \n",
    "    df[\"unique_word_count\"] = df[\"answer\"].apply(count_unique_words)\n",
    "    \n",
    "    # 4) Apply it to your DataFrame:\n",
    "    df[\"token_count\"] = df[\"answer\"].apply(count_tokens_tiktoken)\n",
    "    # print(df[[\"country\", \"token_count\"]])\n",
    "    \n",
    "    df[\"avg_word_length\"] = df[\"answer\"].apply(avg_word_length)\n",
    "    \n",
    "    df[[\"sentence_count\", \"avg_sentence_length\"]] = df[\"answer\"]\\\n",
    "    .apply(lambda s: pd.Series(sentence_stats(s)))\n",
    "    \n",
    "    df[\"sentiment_polarity\"] = df[\"answer\"].apply(sentiment_textblob)\n",
    "    \n",
    "    # Expand your DataFrame:\n",
    "    scores_df = df[\"answer\"].apply(lambda s: pd.Series(readability_scores(s)))\n",
    "    df = pd.concat([df, scores_df], axis=1)\n",
    "    \n",
    "    # 3. Apply it to your DataFrame column (for example, 'text' or 'trimmed_text')\n",
    "    df[\"emoji_count\"] = df[\"answer\"].apply(count_emojis)\n",
    "    \n",
    "    df[\"ib_count\"] = df[\"answer\"].apply(count_ib_acronym)\n",
    "    \n",
    "    df[\"keywords\"] = df[\"answer\"].apply(lambda s: count_specific_terms(s, keyWords))\n",
    "    tempDF = pd.DataFrame(df[[\"country\", \"keywords\"]])\n",
    "    \n",
    "    # Expand the 'keywords' dictionary into separate columns\n",
    "    keywords_expanded = df[\"keywords\"].apply(pd.Series)\n",
    "    \n",
    "    # Concatenate the expanded columns to the original DataFrame (excluding 'keywords')\n",
    "    df = pd.concat([df.drop(columns=[\"keywords\"]), keywords_expanded], axis=1)\n",
    "    \n",
    "    # Display summary statistics for numeric columns in df\n",
    "    df[['flesch_reading_ease']].describe()\n",
    "    \n",
    "    additional_data = pd.read_csv(\"member_state_auths_2025-03-14.csv\")\n",
    "    \n",
    "    joined_df = pd.merge(df, additional_data, left_on=\"country\", right_on=\"Member State\", how=\"left\")\n",
    "    joined_df = joined_df.drop(columns=[\"Scope Note\", \"French\", \"Spanish\", \"Arabic\", \"Chinese\", \"Russian\", \"M49 Code\"])\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723d142",
   "metadata": {},
   "source": [
    "# Combining columns into defined groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea88e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineToGroups(groups: dict, df):\n",
    "    # Work on a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    for groupName, group in groups.items():\n",
    "        # Only proceed if all columns in the group exist in the dataframe\n",
    "        existing_cols = [col for col in group if col in df.columns]\n",
    "        \n",
    "        if existing_cols:  # Only create group if at least one column exists\n",
    "            # Create the sum with a temporary name to avoid conflicts\n",
    "            temp_col_name = f\"_temp_{groupName}\"\n",
    "            df[temp_col_name] = df[existing_cols].sum(axis=1)\n",
    "            \n",
    "            # Drop the original columns\n",
    "            df = df.drop(columns=existing_cols)\n",
    "            \n",
    "            # Rename the temporary column to the final group name\n",
    "            df = df.rename(columns={temp_col_name: groupName})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e242",
   "metadata": {},
   "source": [
    "# Combining the dataframes to compute averages for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e96170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_multiple_dataframes_by_country(dataframes, country_col='country'):\n",
    "    \"\"\"\n",
    "    Create a new dataframe with averages of numerical columns for each country\n",
    "    from multiple dataframes with identical structure.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: list of pandas DataFrames with identical structure\n",
    "    country_col: string, name of the country column (default: 'country')\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with averaged values for each country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Method 1: Concatenate all dataframes and group by country\n",
    "    # This is the most efficient approach for multiple dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Identify numerical columns (excluding the country column)\n",
    "    numerical_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Group by country and calculate mean for numerical columns\n",
    "    # For non-numerical columns, take the first value (assuming they're identical)\n",
    "    agg_dict = {}\n",
    "    \n",
    "    for col in combined_df.columns:\n",
    "        if col == country_col:\n",
    "            continue  # Skip the grouping column\n",
    "        elif col in numerical_cols:\n",
    "            agg_dict[col] = 'mean'  # Average numerical columns\n",
    "        else:\n",
    "            agg_dict[col] = 'first'  # Take first value for non-numerical columns\n",
    "    \n",
    "    averaged_df = combined_df.groupby(country_col).agg(agg_dict).reset_index()\n",
    "    \n",
    "    return averaged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcfa65",
   "metadata": {},
   "source": [
    "# Defining groups and keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b25f3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords = [\n",
    "    \"personal\", \"tailor\", \"htx\", \"stx\", \"hf\", \"hhx\", \"10\", \"fgu\", \"eux\", \"eud\", \"?\", \"!\", \"vet\", \"erhverv\", \"university\", \"if you\", \"uu-vejleder\", \"background\", \"hobb\", \"goal\", \"interest\", \"gymnasium\", \"upper secondary\", \"high school\", \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"carpent\", \"joiner\", \"electric\", \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "]\n",
    "\n",
    "groups = {\n",
    "    \"grammatical_analysis\":[\n",
    "        'unique_word_count', 'token_count', \"emoji_count\"\n",
    "    ],\n",
    "    \"academic\": [\n",
    "        \"stx\", \"htx\", \"hhx\", \"hf\", \"gymnasium\", \"upper secondary\", \"high school\", \n",
    "        \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"university\"\n",
    "    ],\n",
    "    \"vocational\": [\n",
    "        \"fgu\", \"eux\", \"eud\", \"vet\", \"erhverv\", \"carpent\", \"joiner\", \"electric\", \n",
    "        \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \n",
    "        \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \n",
    "        \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \n",
    "        \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \n",
    "        \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \n",
    "        \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "    ],\n",
    "    \"userConsiderations\": [\n",
    "        \"?\", \"if you\", \"uu-vejleder\"\n",
    "    ],\n",
    "    \"background\": [\n",
    "        \"background\", \"hobb\", \"goal\", \"interest\"\n",
    "    ],\n",
    "    \"international\": [\n",
    "        \"ib_count\"\n",
    "    ],\n",
    "\n",
    "}\n",
    "\n",
    "# Keywords not assigned to any group (remaining):\n",
    "unassigned = [\n",
    "    \"personal\", \"tailor\", \"10\", \"!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6df24",
   "metadata": {},
   "source": [
    "# Use of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3119b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = loadData(keyWords=keyWords, folder_path=\"countries1\")\n",
    "df2 = loadData(keyWords=keyWords, folder_path=\"countries2\")\n",
    "df3 = loadData(keyWords=keyWords, folder_path=\"countries3\")\n",
    "df4 = loadData(keyWords=keyWords, folder_path=\"countries4\")\n",
    "df5 = loadData(keyWords=keyWords, folder_path=\"countries5\")\n",
    "\n",
    "# df.to_csv(\"answer_data.csv\", index=False)\n",
    "df1_new = combineToGroups(groups, df1)\n",
    "df2_new = combineToGroups(groups, df2)\n",
    "df3_new = combineToGroups(groups, df3)\n",
    "df4_new = combineToGroups(groups, df4)\n",
    "df5_new = combineToGroups(groups, df5)\n",
    "all_df = average_multiple_dataframes_by_country([df1_new, df2_new, df3_new, df4_new, df5_new])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8baa66",
   "metadata": {},
   "source": [
    "# Drop text columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d23b59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = all_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c459680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'text', 'answer', 'word_count', 'avg_word_length',\n",
       "       'sentence_count', 'avg_sentence_length', 'sentiment_polarity',\n",
       "       'flesch_reading_ease', 'personal', 'tailor', '10', '!', 'Member State',\n",
       "       'ISO Code', 'Start date', 'End date', 'Other Names', 'Earlier Name',\n",
       "       'Later Name', 'Geographic Term', 'Membership Document Symbol',\n",
       "       'grammatical_analysis', 'academic', 'vocational', 'userConsiderations',\n",
       "       'background', 'international'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e756ab47",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['text', 'answer', 'word_count', 'avg_word_length', 'avg_sentence_length', 'sentence_count', 'ISO Code', 'Start date', 'End date', 'Other Names', 'Earlier Name', 'Later Name', 'Geographic Term'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[100]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test = \u001b[43mtest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manswer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mword_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mavg_word_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mavg_sentence_length\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence_count\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mISO Code\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStart date\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEnd date\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOther Names\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEarlier Name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mLater Name\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mGeographic Term\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMember State\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:7098\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7097\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7098\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7099\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7100\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['text', 'answer', 'word_count', 'avg_word_length', 'avg_sentence_length', 'sentence_count', 'ISO Code', 'Start date', 'End date', 'Other Names', 'Earlier Name', 'Later Name', 'Geographic Term'] not found in axis\""
     ]
    }
   ],
   "source": [
    "test = test.drop(['text', \"answer\", \"word_count\", \"avg_word_length\", \"avg_sentence_length\", \"sentence_count\", \"ISO Code\", \"Start date\", 'End date', 'Other Names', 'Earlier Name', 'Later Name', 'Geographic Term', 'Member State'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2942bac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['country', 'sentiment_polarity', 'flesch_reading_ease', 'personal',\n",
       "       'tailor', '10', '!', 'Member State', 'Membership Document Symbol',\n",
       "       'grammatical_analysis', 'academic', 'vocational', 'userConsiderations',\n",
       "       'background', 'international'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "42bf0a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Afghanistan\n",
       "1                                 Albania\n",
       "2                                 Algeria\n",
       "3                                 Andorra\n",
       "4                                  Angola\n",
       "                      ...                \n",
       "188    Venezuela (Bolivarian Republic of)\n",
       "189                              Viet Nam\n",
       "190                                 Yemen\n",
       "191                                Zambia\n",
       "192                              Zimbabwe\n",
       "Name: Member State, Length: 193, dtype: object"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['Member State']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
