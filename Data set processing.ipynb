{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8745ba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import tiktoken\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebeb12e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    :param pdf_path: Path to the PDF file.\n",
    "    :return: Extracted text as a string.\n",
    "    \"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PDF: {e}\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29218339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unique_words(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # Remove punctuation, lowercase, split on whitespace\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s.lower())\n",
    "    return len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe5a397",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "def count_tokens_tiktoken(s: str) -> int:\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # .encode() returns a list of token‐IDs, so its length is the token count\n",
    "    return len(encoding.encode(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bb6fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    words = re.findall(r\"\\b\\w+\\b\", s)\n",
    "    avg = sum(len(w) for w in words) / max(len(words), 1)\n",
    "    return round(avg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b1a004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_stats(s: str):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return (0, 0.0)\n",
    "    # Split the text into sentences using regex\n",
    "    sentences = re.split(r\"[.!?]+\", s.strip())\n",
    "    # Remove empty sentences and strip whitespace\n",
    "    sentences = [sent.strip() for sent in sentences if sent.strip()]\n",
    "    count = len(sentences)\n",
    "    if count == 0:\n",
    "        return (0, 0.0)\n",
    "    total_words = sum(len(sent.split()) for sent in sentences)\n",
    "    return count, round(total_words / count, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d6eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Return a tuple of form (polarity, subjectivity ) \n",
    "where polarity is a float within the range [-1.0, 1.0] \n",
    "and subjectivity is a float within the range [0.0, 1.0] \n",
    "where 0.0 is very objective and 1.0 is very subjective.\n",
    "\"\"\"\n",
    "\n",
    "def sentiment_textblob(s: str) -> float:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return 0.0\n",
    "    return TextBlob(s).sentiment.polarity  # range [-1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60cbaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_scores(s: str) -> dict:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return {\"flesch_reading_ease\": 0.0}\n",
    "    return {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(s)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18b5f67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\n",
    "    \"[\"                      # start character class\n",
    "    \"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    \"\\U0001F300-\\U0001F5FF\"  # Misc Symbols & Pictographs\n",
    "    \"\\U0001F680-\\U0001F6FF\"  # Transport & Map\n",
    "    \"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "    \"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "    \"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols & Pictographs\n",
    "    \"\\U00002600-\\U000026FF\"  # Misc Symbols\n",
    "    \"\\U0001F700-\\U0001F77F\"  # Alchemical Symbols\n",
    "    \"]+\", \n",
    "    flags=re.UNICODE\n",
    ")\n",
    "\n",
    "def count_emojis(s: str) -> int:\n",
    "    return len(emoji_pattern.findall(s or \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "292cf7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ib_acronym(s: str) -> int:\n",
    "    \"\"\"\n",
    "    Count only standalone occurrences of 'IB' (case‐insensitive),\n",
    "    including when wrapped in parentheses like '(IB)'.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return 0\n",
    "    # \\b ensures IB is not part of a longer word. \n",
    "    # Flags=re.IGNORECASE lets us catch 'IB', 'ib', 'Ib', etc.\n",
    "    return len(re.findall(r\"\\bIB\\b\", s, flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "320fc478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed025549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_specific_terms(s: str, terms: list) -> dict:\n",
    "    if not isinstance(s, str):\n",
    "        return {term: 0 for term in terms}\n",
    "    lower_s = s.lower()\n",
    "    return {term: lower_s.count(term) for term in terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6646259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(keyWords: list, folder_path  = \"responses\"):\n",
    "    data = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(folder_path, filename)\n",
    "            extracted_text = extract_text_from_pdf(full_path)\n",
    "            data.append({\n",
    "                \"country\": os.path.splitext(filename)[0],\n",
    "                \"text\": extracted_text\n",
    "            })\n",
    "    \n",
    "    prompt = r\"(?s).*?What educational path would you recommend for me\\?\"\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create a new column (or overwrite) with only the text after that question:\n",
    "    df[\"answer\"] = df[\"text\"].apply(\n",
    "        lambda t: re.sub(prompt, \"\", t)\n",
    "    )\n",
    "    \n",
    "    pattern = r\"Printed using ChatGPT to PDF, powered by PDFCrowd HTML to PDF API\\. \\d+/\\d+\"\n",
    "    \n",
    "    df[\"answer\"] = df[\"answer\"].str.replace(pattern, \"\", regex=True)\n",
    "    \n",
    "    df[\"word_count\"] = df[\"answer\"].apply(lambda s: len(s.split()) if isinstance(s, str) else 0)\n",
    "    \n",
    "    df[\"unique_word_count\"] = df[\"answer\"].apply(count_unique_words)\n",
    "    \n",
    "    # 4) Apply it to your DataFrame:\n",
    "    df[\"token_count\"] = df[\"answer\"].apply(count_tokens_tiktoken)\n",
    "    # print(df[[\"country\", \"token_count\"]])\n",
    "    \n",
    "    df[\"avg_word_length\"] = df[\"answer\"].apply(avg_word_length)\n",
    "    \n",
    "    df[[\"sentence_count\", \"avg_sentence_length\"]] = df[\"answer\"]\\\n",
    "    .apply(lambda s: pd.Series(sentence_stats(s)))\n",
    "    \n",
    "    df[\"sentiment_polarity\"] = df[\"answer\"].apply(sentiment_textblob)\n",
    "    \n",
    "    # Expand your DataFrame:\n",
    "    scores_df = df[\"answer\"].apply(lambda s: pd.Series(readability_scores(s)))\n",
    "    df = pd.concat([df, scores_df], axis=1)\n",
    "    \n",
    "    # 3. Apply it to your DataFrame column (for example, 'text' or 'trimmed_text')\n",
    "    df[\"emoji_count\"] = df[\"answer\"].apply(count_emojis)\n",
    "    \n",
    "    df[\"ib_count\"] = df[\"answer\"].apply(count_ib_acronym)\n",
    "    \n",
    "    df[\"keywords\"] = df[\"answer\"].apply(lambda s: count_specific_terms(s, keyWords))\n",
    "    tempDF = pd.DataFrame(df[[\"country\", \"keywords\"]])\n",
    "    \n",
    "    # Expand the 'keywords' dictionary into separate columns\n",
    "    keywords_expanded = df[\"keywords\"].apply(pd.Series)\n",
    "    \n",
    "    # Concatenate the expanded columns to the original DataFrame (excluding 'keywords')\n",
    "    df = pd.concat([df.drop(columns=[\"keywords\"]), keywords_expanded], axis=1)\n",
    "    \n",
    "    # Display summary statistics for numeric columns in df\n",
    "    df[['flesch_reading_ease']].describe()\n",
    "    \n",
    "    additional_data = pd.read_csv(\"member_state_auths_2025-03-14.csv\")\n",
    "    \n",
    "    joined_df = pd.merge(df, additional_data, left_on=\"country\", right_on=\"Member State\", how=\"left\")\n",
    "    joined_df = joined_df.drop(columns=[\"Scope Note\", \"French\", \"Spanish\", \"Arabic\", \"Chinese\", \"Russian\", \"M49 Code\"])\n",
    "    \n",
    "    return joined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723d142",
   "metadata": {},
   "source": [
    "# Combining columns into defined groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea88e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineToGroups(groups: dict, df):\n",
    "    # Work on a copy to avoid modifying the original dataframe\n",
    "    df = df.copy()\n",
    "    \n",
    "    for groupName, group in groups.items():\n",
    "        # Only proceed if all columns in the group exist in the dataframe\n",
    "        existing_cols = [col for col in group if col in df.columns]\n",
    "        \n",
    "        if existing_cols:  # Only create group if at least one column exists\n",
    "            # Create the sum with a temporary name to avoid conflicts\n",
    "            temp_col_name = f\"_temp_{groupName}\"\n",
    "            df[temp_col_name] = df[existing_cols].sum(axis=1)\n",
    "            \n",
    "            # Drop the original columns\n",
    "            df = df.drop(columns=existing_cols)\n",
    "            \n",
    "            # Rename the temporary column to the final group name\n",
    "            df = df.rename(columns={temp_col_name: groupName})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74e242",
   "metadata": {},
   "source": [
    "# Combining the dataframes to compute averages for each country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e96170c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_multiple_dataframes_by_country(dataframes, country_col='country'):\n",
    "    \"\"\"\n",
    "    Create a new dataframe with averages of numerical columns for each country\n",
    "    from multiple dataframes with identical structure.\n",
    "    \n",
    "    Parameters:\n",
    "    dataframes: list of pandas DataFrames with identical structure\n",
    "    country_col: string, name of the country column (default: 'country')\n",
    "    \n",
    "    Returns:\n",
    "    pandas DataFrame with averaged values for each country\n",
    "    \"\"\"\n",
    "    \n",
    "    # Method 1: Concatenate all dataframes and group by country\n",
    "    # This is the most efficient approach for multiple dataframes\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    \n",
    "    # Identify numerical columns (excluding the country column)\n",
    "    numerical_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Group by country and calculate mean for numerical columns\n",
    "    # For non-numerical columns, take the first value (assuming they're identical)\n",
    "    agg_dict = {}\n",
    "    \n",
    "    for col in combined_df.columns:\n",
    "        if col == country_col:\n",
    "            continue  # Skip the grouping column\n",
    "        elif col in numerical_cols:\n",
    "            agg_dict[col] = 'mean'  # Average numerical columns\n",
    "        else:\n",
    "            agg_dict[col] = 'first'  # Take first value for non-numerical columns\n",
    "    \n",
    "    averaged_df = combined_df.groupby(country_col).agg(agg_dict).reset_index()\n",
    "    \n",
    "    return averaged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefcfa65",
   "metadata": {},
   "source": [
    "# Defining groups and keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b25f3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyWords = [\n",
    "    \"personal\", \"tailor\", \"htx\", \"stx\", \"hf\", \"hhx\", \"10\", \"fgu\", \"eux\", \"eud\", \"?\", \"!\", \"vet\", \"erhverv\", \"university\", \"if you\", \"uu-vejleder\", \"background\", \"hobb\", \"goal\", \"interest\", \"gymnasium\", \"upper secondary\", \"high school\", \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"carpent\", \"joiner\", \"electric\", \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "]\n",
    "\n",
    "groups = {\n",
    "    \"academic\": [\n",
    "        \"stx\", \"htx\", \"hhx\", \"hf\", \"gymnasium\", \"upper secondary\", \"high school\", \n",
    "        \"academic\", \"exam\", \"graduation\", \"GPA\", \"read\", \"preparation\", \"university\"\n",
    "    ],\n",
    "    \"vocational\": [\n",
    "        \"fgu\", \"eux\", \"eud\", \"vet\", \"erhverv\", \"carpent\", \"joiner\", \"electric\", \n",
    "        \"plumb\", \"brick\", \"mechanic\", \"blacksmith\", \"metalwork\", \"machinist\", \n",
    "        \"weld\", \"construction\", \"technician\", \"hair\", \"beaut\", \"cosmetolog\", \n",
    "        \"skincare\", \"barber\", \"makeup\", \"styli\", \"chef\", \"cook\", \"baker\", \n",
    "        \"waiter\", \"waitress\", \"kitchen\", \"cater\", \"nurs\", \"child\", \"pedagog\", \n",
    "        \"elder\", \"disab\", \"clerk\", \"shop\", \"warehouse\", \"farm\", \"garden\", \n",
    "        \"animal\", \"forest\", \"zoo\", \"sosu\"\n",
    "    ],\n",
    "    \"userConsiderations\": [\n",
    "        \"?\", \"if you\", \"uu-vejleder\"\n",
    "    ],\n",
    "    \"background\": [\n",
    "        \"background\", \"hobb\", \"goal\", \"interest\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Keywords not assigned to any group (remaining):\n",
    "unassigned = [\n",
    "    \"personal\", \"tailor\", \"10\", \"!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6df24",
   "metadata": {},
   "source": [
    "# Use of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3119b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = loadData(keyWords=keyWords, folder_path=\"responses\")\n",
    "df2 = loadData(keyWords=keyWords, folder_path=\"responses\")\n",
    "# df.to_csv(\"answer_data.csv\", index=False)\n",
    "df1_new = combineToGroups(groups, df1)\n",
    "df2_new = combineToGroups(groups, df2)\n",
    "together = average_multiple_dataframes_by_country([df1_new, df2_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "972c926a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Passing a dict as an indexer is not supported. Use a list instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1_new \u001b[38;5;241m=\u001b[39m \u001b[43mcombineToGroups\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df1\u001b[38;5;241m.\u001b[39mloc[df1[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfghanistan\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m, in \u001b[0;36mcombineToGroups\u001b[1;34m(groups, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombineToGroups\u001b[39m(groups : \u001b[38;5;28mdict\u001b[39m, df):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m groupName, group \u001b[38;5;129;01min\u001b[39;00m groups\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 3\u001b[0m         df[group[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m      5\u001b[0m         df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{group[\u001b[38;5;241m0\u001b[39m]: groupName})\n",
      "File \u001b[1;32mc:\\Users\\NTres\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3714\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m-> 3714\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3715\u001b[0m     key \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(key)\n\u001b[0;32m   3716\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\NTres\\miniconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2627\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a set as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2620\u001b[0m     )\n\u001b[0;32m   2622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2623\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m   2624\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2625\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2626\u001b[0m ):\n\u001b[1;32m-> 2627\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2628\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing a dict as an indexer is not supported. Use a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2629\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: Passing a dict as an indexer is not supported. Use a list instead."
     ]
    }
   ],
   "source": [
    "df1_new = combineToGroups(groups, df1)\n",
    "df1.loc[df1[\"country\"] == \"Afghanistan\"][\"htx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5509cd94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['!', '10', 'Earlier Name', 'End date', 'Geographic Term',\n",
       "       'ISO Code', 'Later Name', 'Member State',\n",
       "       'Membership Document Symbol', 'Other Names', 'Start date',\n",
       "       'academic', 'answer', 'avg_sentence_length', 'avg_word_length',\n",
       "       'background', 'country', 'emoji_count', 'flesch_reading_ease',\n",
       "       'ib_count', 'personal', 'sentence_count', 'sentiment_polarity',\n",
       "       'tailor', 'text', 'token_count', 'unique_word_count',\n",
       "       'userConsiderations', 'vocational', 'word_count'], dtype='<U26')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(list(df1_new.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5692db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    21\n",
       "Name: academic, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_new.loc[df1_new[\"country\"] == \"Afghanistan\"][\"academic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1719af22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_new.iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3689c2af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      21.0\n",
       "1      23.0\n",
       "2      18.0\n",
       "3      17.0\n",
       "4      11.0\n",
       "       ... \n",
       "188    25.0\n",
       "189    23.0\n",
       "190    22.0\n",
       "191    22.0\n",
       "192    26.0\n",
       "Name: academic, Length: 193, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "together[\"academic\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
